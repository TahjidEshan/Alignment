{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM_v5.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"luMjNtsqifzl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1594955714443,"user_tz":360,"elapsed":3833,"user":{"displayName":"Tahjid Ashfaque Mostafa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiATFk51SguXn70zJprd1GcXHxsI06mHHZBBMf=s64","userId":"04530508710412041120"}},"outputId":"07ec14bf-d7ff-440b-a6fb-9ab53e552520"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from numpy import random\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from torch.utils.data import DataLoader\n","from collections import deque\n","import numpy as np\n","import multiprocessing\n","import time\n","from numpy import array\n","import pandas as pd\n","from torch.utils.data import TensorDataset, DataLoader\n","torch.manual_seed(1)\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"8y4Ls1vqi5dn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594955714603,"user_tz":360,"elapsed":3978,"user":{"displayName":"Tahjid Ashfaque Mostafa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiATFk51SguXn70zJprd1GcXHxsI06mHHZBBMf=s64","userId":"04530508710412041120"}}},"source":["from sklearn.preprocessing import MinMaxScaler\n","samplesize = 5\n","#rotate a list by given offset, (negative offset, right to left) \n","def rotateList(items, offset):\n","    return np.roll(items,offset)\n","\n","def secondToMillisecond(val):\n","    return val*1000\n","\n","def gaussiantimeseries(limit = 60, interval = 2):\n","    data = {}\n","    for t in range(0,limit,interval):\n","        data[t]=random.normal(loc = random.uniform(1,100), scale =1.5, size =1000).astype(float)\n","    return data\n","\n","def createsignal(limit = 60, interval = 2):\n","    signal1, signal2, offset = [],[],[]\n","    limit = secondToMillisecond(limit)\n","    offset = random.randint(-10,10)\n","    data = gaussiantimeseries(limit, interval)\n","    for t in data:\n","        dist = data[t]\n","        signal1.append(random.choice(dist,size=samplesize,replace=True))\n","        signal2.append(random.choice(dist,size=samplesize,replace=True))\n","    # print(array(signal1).shape)\n","    return array(signal1), array(signal2), array(rotateList(signal2,offset)), offset\n","\n","def createsignals(num = 50, limit = 60, interval = 2, normalize = False):\n","    input_seq = []\n","    data = []\n","    for i in range(num):\n","        sig1, sig2original, sig2, offset = createsignal(limit = limit)\n","        data.append([sig1, sig2original, sig2, offset])\n","        train_data = np.hstack((sig1,sig2)).astype(float)\n","        #train data without normalization\n","        if not normalize:\n","            train_data = torch.FloatTensor(train_data)\n","            input_seq.append((train_data, torch.FloatTensor([offset])))\n","        else:\n","            scaler = MinMaxScaler(feature_range=(-1, 1))\n","            train_data_normalized = scaler.fit_transform(train_data)\n","            train_data_normalized = torch.FloatTensor(train_data_normalized)\n","            input_seq.append((train_data_normalized, torch.FloatTensor([offset])))\n","    return input_seq, data\n","\n","def createsignalsfortensor(num = 50, limit = 60, interval = 2, normalize = False):\n","    input_seq = []\n","    labels = []\n","    for i in range(num):\n","        sig1, sig2original, sig2, offset = createsignal(limit = limit)\n","        train_data = np.hstack((sig1,sig2)).astype(float)\n","        #train data without normalization\n","        if not normalize:\n","            train_data = torch.FloatTensor(train_data)\n","            input_seq.append(train_data)\n","            labels.append(torch.FloatTensor([offset]))\n","        else:\n","            scaler = MinMaxScaler(feature_range=(-1, 1))\n","            train_data_normalized = scaler.fit_transform(train_data)\n","            train_data_normalized = torch.FloatTensor(train_data_normalized)\n","            input_seq.append(train_data_normalized)\n","            labels.append(torch.FloatTensor([offset]))\n","    return input_seq, labels\n","# print(createsignal())"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"1GUBGQC9i-7N","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594959355545,"user_tz":360,"elapsed":3644913,"user":{"displayName":"Tahjid Ashfaque Mostafa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiATFk51SguXn70zJprd1GcXHxsI06mHHZBBMf=s64","userId":"04530508710412041120"}}},"source":["train_input, train_label = createsignalsfortensor(num=1000, limit = 60, interval = 1, normalize = False)\n","# val_input, val_label = createsignalsfortensor(num=1)\n","test_input, test_label = createsignalsfortensor()"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"nywykOPfkbvq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594959355548,"user_tz":360,"elapsed":3644910,"user":{"displayName":"Tahjid Ashfaque Mostafa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiATFk51SguXn70zJprd1GcXHxsI06mHHZBBMf=s64","userId":"04530508710412041120"}}},"source":["a,b,c,d=createsignal(limit=1)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgM5sXMbplzI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594959355549,"user_tz":360,"elapsed":3644902,"user":{"displayName":"Tahjid Ashfaque Mostafa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiATFk51SguXn70zJprd1GcXHxsI06mHHZBBMf=s64","userId":"04530508710412041120"}},"outputId":"8697b0c1-e720-4c17-d36f-505e2bd200b0"},"source":["a.shape"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(500, 5)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"Ez-beYT4mFs3","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594959355549,"user_tz":360,"elapsed":3644891,"user":{"displayName":"Tahjid Ashfaque Mostafa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiATFk51SguXn70zJprd1GcXHxsI06mHHZBBMf=s64","userId":"04530508710412041120"}}},"source":["class CustomDataset(torch.utils.data.Dataset):\n","  def __init__(self, data, labels):\n","        self.labels = labels\n","        self.data = data\n","\n","  def __len__(self):\n","        return len(self.data)\n","\n","  def __getitem__(self, index):\n","        X = self.data[index]\n","        y = self.labels[index]\n","\n","        return X, y"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"XQa3b6DmkXH8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594959355550,"user_tz":360,"elapsed":3644886,"user":{"displayName":"Tahjid Ashfaque Mostafa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiATFk51SguXn70zJprd1GcXHxsI06mHHZBBMf=s64","userId":"04530508710412041120"}}},"source":["train_data = CustomDataset(train_input,train_label)\n","# val_data = CustomDataset(test_input,test_label)\n","test_data = CustomDataset(test_input,test_label)\n","\n","batch_size = 10\n","\n","train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n","# val_loader = DataLoader(val_data, shuffle=True, batch_size = batch_size)\n","test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"8bmYWFR1n8WW","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594959355551,"user_tz":360,"elapsed":3644881,"user":{"displayName":"Tahjid Ashfaque Mostafa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiATFk51SguXn70zJprd1GcXHxsI06mHHZBBMf=s64","userId":"04530508710412041120"}}},"source":["class LSTM(nn.Module):\n","    def __init__(self, input_size=2, output_size = 1, hidden_dim = 512, n_layers = 2, drop_prob=0.5):\n","        super(LSTM, self).__init__()\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        self.hidden_dim = hidden_dim\n","        \n","\n","        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n","        self.dropout = nn.Dropout(drop_prob)\n","        self.fc = nn.Linear(hidden_dim, output_size)\n","        # self.sigmoid = nn.Sigmoid()\n","        \n","    def forward(self, x, hidden):\n","        batch_size = x.size(0)\n","        # x = x.long()\n","        lstm_out, hidden = self.lstm(x, hidden)\n","        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n","        \n","        # lstm_out = self.dropout(lstm_out)\n","        out = self.fc(lstm_out)\n","        # out = self.sigmoid(out)\n","        \n","        out = out.view(batch_size, -1)\n","        out = out[:,-1]\n","        # print(out.shape)\n","        return out, hidden\n","    \n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n","        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n","                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n","        return hidden"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"I-3n62r_pcWM","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594967176462,"user_tz":360,"elapsed":383,"user":{"displayName":"Tahjid Ashfaque Mostafa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiATFk51SguXn70zJprd1GcXHxsI06mHHZBBMf=s64","userId":"04530508710412041120"}}},"source":["model = LSTM(samplesize*2)\n","model.to(device)\n","\n","lr = 0.0001\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"wrf3QNZG6C_K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"status":"error","timestamp":1594967833034,"user_tz":360,"elapsed":583,"user":{"displayName":"Tahjid Ashfaque Mostafa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiATFk51SguXn70zJprd1GcXHxsI06mHHZBBMf=s64","userId":"04530508710412041120"}},"outputId":"743940f2-0faa-4fd2-a42c-229537a949e2"},"source":["epochs = 15\n","counter = 0\n","print_every = 100\n","clip = 5\n","valid_loss_min = np.Inf\n","\n","model.train()\n","for i in range(epochs):\n","    # print(batch_size)\n","    print(\"Epoch {}/{}\".format(i+1, epochs))\n","    print('-' * 20)\n","    epoch_loss = 0.0\n","    h = model.init_hidden(batch_size)\n","    model.zero_grad()\n","    for inputs, labels in train_loader:\n","        # print(inputs.shape)\n","        counter += 1\n","        h = tuple([e.data for e in h])\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        output, h = model(inputs, h)\n","        # print(\"a\",output.shape)\n","        # print(\"b\",labels.squeeze().shape)\n","        optimizer.zero_grad()\n","        loss = criterion(output, labels.squeeze())\n","        # print(output)\n","        # print(labels)\n","        # break\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        optimizer.step()\n","        epoch_loss += output.shape[0] * loss.item()\n","        if counter%print_every == 0:\n","        #     # print(batch_size)\n","        #     val_h = model.init_hidden(batch_size)\n","        #     val_losses = []\n","        #     model.eval()\n","        #     for inp, lab in val_loader:\n","        #         val_h = tuple([each.data for each in val_h])\n","        #         inp, lab = inp.to(device), lab.to(device)\n","        #         out, val_h = model(inp, val_h)\n","        #         val_loss = criterion(out, lab.squeeze())\n","        #         val_losses.append(val_loss.item())\n","                \n","            # model.train()\n","            print(\"Epoch: {}/{}\".format(i+1, epochs),\n","                  \"Step: {}\".format(counter),\n","                  \"Loss: {:.6f}\".format(loss.item()))\n","            #       \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n","            # if np.mean(val_losses) <= valid_loss_min:\n","            #     # torch.save(model.state_dict(), './state_dict.pt')\n","            #     print('Validation loss decreased ({:.6f} --> {:.6f}).'.format(valid_loss_min,np.mean(val_losses)))\n","            #     valid_loss_min = np.mean(val_losses)\n","    # break\n","    print(\"Epoch {} Loss {}\".format(i+1, epoch_loss / len(train_input)))"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-ead8d1c9ec56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvalid_loss_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}]},{"cell_type":"code","metadata":{"id":"TcGi6R6Uhuxt","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1594967163244,"user_tz":360,"elapsed":11452547,"user":{"displayName":"Tahjid Ashfaque Mostafa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiATFk51SguXn70zJprd1GcXHxsI06mHHZBBMf=s64","userId":"04530508710412041120"}}},"source":["test_losses = []\n","num_correct = 0\n","h = model.init_hidden(batch_size)\n","\n","model.eval()\n","for inputs, labels in test_loader:\n","    h = tuple([each.data for each in h])\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    # print(inputs)\n","    output, h = model(inputs, h)\n","    print(f\"Offset {labels.squeeze().data}, Prediction {output.data}\")\n","    test_loss = criterion(output, labels.squeeze())\n","    test_losses.append(test_loss.item())\n","    pred = torch.round(output)  # Rounds the output to 0/1\n","    correct_tensor = pred.eq(labels.squeeze().view_as(pred))\n","    correct = np.squeeze(correct_tensor.cpu().numpy())\n","    num_correct += np.sum(correct)\n","\n","print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n","test_acc = num_correct/len(test_loader.dataset)\n","print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"],"execution_count":null,"outputs":[]}]}